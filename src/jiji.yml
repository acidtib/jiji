# Project name (required) used for organizing services
# All audit logs and deployment locks will be stored under .jiji/{project}/ on remote hosts
project: myproject

# Builder Configuration
# ====================
# Controls how container images are built and where they're stored
# Supports both local builds (on your machine) and remote builds (on dedicated build servers)
#
# Note: Build-specific options like context, dockerfile, and args are configured per-service
# in the services section under the 'build' option. The builder section contains global
# settings that apply to all builds.
#
# Builder Use Cases:
# 1. Local Development: Build images locally and use local registry with SSH port forwarding
# 2. Remote Building: Offload builds to powerful remote servers for faster compilation
# 3. CI/CD Integration: Push to remote registries for production deployment
# 4. Multi-architecture: Use remote builders for cross-platform image builds

builder:
  # Container engine for building (docker or podman)
  # Options: docker, podman
  engine: podman

  # Build location - determines where images are built
  local: true

  # Remote builder SSH connection (required when local: false)
  # Enables building on remote servers with more resources or different architectures
  # Format: ssh://[user@]hostname[:port]
  # Examples:
  # remote: ssh://builder@192.168.1.50:22
  # remote: ssh://root@build-server.example.com
  # remote: ssh://ubuntu@build-arm64.local:2222

  # Enable build cache (default: true)
  # Set to false to force clean builds (useful for CI/CD or debugging)
  cache: true

  # Container Registry Configuration
  # ===============================
  # Determines where built images are stored and retrieved from
  registry:
    # Registry type: "local" or "remote"
    type: local

    # Local Registry Settings (type: local)
    # Creates a local registry with SSH port forwarding to deployment hosts
    # Perfect for development and small deployments
    port: 6767

    # Remote Registry Settings (type: remote)
    # Use when deploying to production or sharing images across teams
    # Examples: Docker Hub, GitHub Container Registry
    # server: registry.example.com:5000
    # server: ghcr.io
    # server: docker.io
    # username: myuser
    # password: "${REGISTRY_PASSWORD}"  # Environment variable substitution supported

# Builder Configuration Examples:
# ===============================
#
# Example 1: Local Development Setup
# builder:
#   engine: podman
#   local: true
#   cache: true
#   registry:
#     type: local
#     port: 6767
#
# Example 2: Remote Building with Local Registry
# builder:
#   engine: docker
#   local: false
#   remote: ssh://builder@192.168.1.100:22
#   cache: true
#   registry:
#     type: local
#     port: 6767
#
# Example 3: Production with Remote Registry (Docker Hub)
# builder:
#   engine: docker
#   local: true
#   cache: false  # Always fresh builds in CI
#   registry:
#     type: remote
#     server: docker.io
#     username: myusername
#     password: "${DOCKER_HUB_TOKEN}"
#
# Example 4: AWS ECR Registry
# builder:
#   engine: docker
#   local: true
#   registry:
#     type: remote
#     server: 123456789012.dkr.ecr.us-west-2.amazonaws.com
#     username: AWS
#     password: "${ECR_TOKEN}"
#
# Example 5: Remote Building with Custom Registry Port
# builder:
#   engine: podman
#   local: false
#   remote: ssh://build-server@192.168.1.200:22
#   cache: true
#   registry:
#     type: local
#     port: 6000

# Server Definitions (required)
# ===============================
# Define all servers once by name, then reference them in services using hosts: [name]
# This eliminates duplication and allows per-server SSH configuration overrides
#
# Benefits:
# - Define servers once, reference by name in multiple services
# - Per-server SSH credentials (different users, ports, keys per server)
# - Clear deployment targets - explicit server names in service configs
# - Foundation for future features (server groups, tags, health monitoring)
# - WireGuard peering works between ALL defined servers (not just those with services)
#
# Each server must have:
# - Unique name (DNS-safe: alphanumeric + hyphens, no start/end hyphen, max 63 chars)
# - Unique hostname/IP address (no duplicate hosts across servers)
#
# Per-server SSH overrides (all optional, inherit from global ssh section if not specified):
# - user: SSH username for this specific server
# - port: SSH port for this specific server
# - key_path: Path to SSH private key for this server
# - keys: Array of SSH key paths for this server
# - key_data: Array of inline SSH key data from environment variables
#
# Architecture (arch):
# - amd64 (default): x86_64/AMD64 architecture
# - arm64: ARM64/aarch64 architecture
# - Used for multi-architecture image builds

servers:
  # Production VPS (default settings)
  vps-prod:
    host: 15.204.224.81
    arch: amd64 # Optional, defaults to amd64

  # Home server with custom SSH user
  home-server:
    host: 192.168.1.220
    arch: amd64
    user: ubuntu # Override global SSH user for this server

  # ARM build server with custom port and key
  arm-builder:
    host: 192.168.1.230
    arch: arm64
    port: 2222 # Custom SSH port
    key_path: ~/.ssh/arm_builder_key # Server-specific SSH key

# Server Configuration Examples:
# ===============================
#
# Example 1: Simple Multi-Server Setup
# servers:
#   server1:
#     host: 192.168.1.100
#   server2:
#     host: 192.168.1.101
#
# Example 2: Mixed Architectures
# servers:
#   amd-server:
#     host: 10.0.0.10
#     arch: amd64
#   arm-server:
#     host: 10.0.0.20
#     arch: arm64
#
# Example 3: Different SSH Users Per Server
# servers:
#   ubuntu-box:
#     host: 192.168.1.10
#     user: ubuntu
#   centos-box:
#     host: 192.168.1.20
#     user: centos
#
# Example 4: CI/CD with Build Server
# servers:
#   build-server:
#     host: builder.example.com
#     user: buildbot
#     port: 2222
#   prod-server:
#     host: prod.example.com
#     user: deploy
#
# Migration from Old Format:
# ==========================
# OLD (deprecated):
# services:
#   web:
#     servers:
#       - host: 192.168.1.100
#
# NEW (current):
# servers:
#   server1:
#     host: 192.168.1.100
# services:
#   web:
#     hosts: [server1]

# SSH configuration for remote connections
ssh:
  user: root
  port: 22
  # Optional: Path to SSH private key
  # key_path: ~/.ssh/id_rsa
  # Optional: SSH key passphrase
  # key_passphrase: "your-passphrase"
  # Optional: Connection timeout in seconds (default: 30)
  # connect_timeout: 30
  # Optional: Command timeout in seconds (default: 300)
  # command_timeout: 300
  # Optional: Additional SSH options
  # options:
  #   StrictHostKeyChecking: "no"
  #   UserKnownHostsFile: "/dev/null"

  # Log Level Configuration
  # Controls verbosity of SSH operation logging
  # Options: debug, info, warn, error, fatal (default: error)
  # - debug: Show all SSH operations and detailed connection info
  # - info: Show major operations and status updates
  # - warn: Show warnings and important notices
  # - error: Show only errors and failures (default)
  # - fatal: Show only critical failures
  # log_level: error

  # Multiple Private Keys Support
  # Specify multiple key files (paths are expanded automatically, ~ supported)
  # keys:
  #   - ~/.ssh/id_rsa
  #   - ~/.ssh/deploy_key
  #   - /path/to/special_key

  # Inline Key Data from Environment Variables
  # Useful for CI/CD where keys are stored as secrets
  # key_data:
  #   - SSH_PRIVATE_KEY_1
  #   - SSH_PRIVATE_KEY_2

  # Keys Only Mode - ignore ssh-agent, use only specified keys
  # Set to true to disable ssh-agent and use only keys specified above
  # keys_only: false

  # Concurrency Control
  # Limit concurrent SSH connections to prevent overwhelming servers
  # max_concurrent_starts: 30

  # Connection pool idle timeout in seconds
  # pool_idle_timeout: 900

  # DNS lookup retry attempts (with exponential backoff)
  # dns_retries: 3

  # SSH Proxy/Jump Host Configuration
  # Use either 'proxy' (ProxyJump) or 'proxy_command' (ProxyCommand), not both

  # ProxyJump - Simple bastion/jump host
  # Automatically tunnels SSH connections through the specified proxy host
  # Format: [user@]hostname[:port]
  # Examples:
  # proxy: root@bastion.example.com
  # proxy: bastion.example.com:2222
  # proxy: deploy@jump-host

  # ProxyCommand - Custom proxy command with full control
  # Executes a custom command to establish the SSH connection
  # Use %h for target hostname and %p for target port
  # Examples:
  # proxy_command: "ssh -W %h:%p user@proxy.example.com"
  # proxy_command: "ssh -W %h:%p -i ~/.ssh/bastion_key bastion.example.com"
  # proxy_command: "nc -X connect -x proxy.example.com:3128 %h %p"

  # SSH Configuration File Support
  # Load SSH configuration from ~/.ssh/config files to leverage existing SSH setups
  # This allows Jiji to inherit host-specific settings like ProxyJump, IdentityFile, etc.
  # Jiji configuration takes precedence over SSH config file settings

  # Load SSH config files:
  # config: true                    # Load default files (~/.ssh/config, /etc/ssh/ssh_config)
  # config: "~/.ssh/custom_config"  # Load specific file
  # config:                         # Load multiple files
  #   - ~/.ssh/config
  #   - ~/.ssh/work_config
  # config: false                   # Don't load config files (default)

  # When enabled, Jiji will:
  # - Parse SSH config files for host-specific settings
  # - Apply matching Host patterns (supports wildcards * and ?)
  # - Use ProxyJump/ProxyCommand from config if not set in Jiji
  # - Use IdentityFile from config if no keys specified in Jiji
  # - Apply connection timeouts and other SSH options
  # - Always prioritize Jiji configuration over SSH config file settings

# Private Networking Configuration
# ================================
# Enables secure, encrypted container-to-container communication across servers
# using WireGuard VPN mesh network with automatic service discovery via DNS
#
# Network Architecture:
# - WireGuard: Creates encrypted mesh VPN between all servers
# - Corrosion: Distributed CRDT database for service registry (gossip protocol)
# - CoreDNS: DNS server for service discovery (resolves <service>.jiji to container IPs)
# - Dual-stack: IPv4 (10.210.0.0/16) for containers, IPv6 (fdcc::/16) for management
# - Automatic: Containers are auto-registered on deploy, auto-unregistered on remove
#
# Use Cases:
# - Microservices communication without exposing ports publicly
# - Multi-server deployments with service discovery
# - Database replication across servers
# - Secure inter-service authentication
# - Zero-trust networking with encryption by default
#
# network:
#   # Enable private networking (default: true)
#   enabled: true
#
#   # Cluster CIDR for container networking
#   # Each server gets a /24 subnet (254 usable IPs per server)
#   # Default: 10.210.0.0/16 (supports 256 servers)
#   cluster_cidr: "10.210.0.0/16"
#
#   # Note: Service domain is always 'jiji' - containers reach each other via <service>.jiji
#   # Note: Service discovery method is always 'corrosion' (distributed CRDT-based)
#   # Note: Corrosion gossip port (8787) and API port (8080) are not configurable
#
# Network Configuration Examples:
# ==============================
#
# Example 1: Basic Setup (Recommended)
# network:
#   enabled: true
#   cluster_cidr: "10.210.0.0/16"
#
# Example 2: Custom CIDR for Existing Network
# network:
#   enabled: true
#   cluster_cidr: "172.20.0.0/16"    # Avoid conflicts with existing networks
#
# Example 3: Disable Networking
# network:
#   enabled: false
#
# How It Works:
# ============
# 1. Initialization: Run 'jiji server init' to set up network infrastructure
#    - Installs WireGuard on all servers
#    - Generates keypairs and establishes mesh VPN
#    - Installs Corrosion for distributed service registry
#    - Installs CoreDNS for DNS-based service discovery
#
# 2. Deploy: Run 'jiji deploy' to deploy containers
#    - Containers automatically join the private network
#    - Registered in Corrosion with service name and IP
#    - DNS automatically updated to resolve <service>.jiji
#
# 3. Service Discovery: Containers communicate via DNS
#    - Example: curl http://api-backend.jiji:3000/health
#    - DNS resolves to all healthy container IPs for that service
#    - Load balanced automatically by DNS round-robin
#
# 4. Network Status: Check network health
#    - Run 'jiji network status' to see topology and server status
#
# 5. Teardown: Clean up network (if needed)
#    - Run 'jiji network teardown' to remove all network infrastructure
#
# Service Communication Example:
# ============================
# Given this configuration:
#
# servers:
#   server1:
#     host: 192.168.1.100
#   server2:
#     host: 192.168.1.101
#   server3:
#     host: 192.168.1.102
#
# services:
#   api-backend:
#     hosts: [server1, server2]
#
#   database:
#     hosts: [server3]
#
# The api-backend containers can connect to database via:
#   DATABASE_URL: postgresql://user:pass@database.jiji:5432/myapp
#
# And database can connect back to api via:
#   API_URL: http://api-backend.jiji:3000

# Shared Environment Configuration
# Applied to all services, can be overridden per-service
# environment:
#   # Clear text environment variables
#   clear:
#     APP_ENV: production
#     LOG_LEVEL: info
#     DEBUG: "false"
#   # Secrets loaded from host environment variables
#   secrets:
#     - DATABASE_PASSWORD
#     - API_SECRET_KEY

# Services Configuration
# Services are the deployable units in Jiji - each service represents a containerized application
# Use service filtering during deployment to target specific services or groups
#
# Service Filtering:
# Target specific services with the --services flag:
#   jiji server init --services web-frontend
#   jiji server init --services web-frontend,api-backend
#   jiji server exec "docker ps" --services web-*
#
# Service filters support wildcards (* pattern matching):
#   --services "web-*"           # Matches: web-frontend, web-api, web-worker
#   --services "api-*,worker-*"  # Multiple wildcard patterns
#
# Combine with host filtering for precise targeting:
#   jiji server exec "systemctl status docker" --services web-* --hosts 192.168.1.100
#
# Image Retention Configuration:
# =============================
# Jiji automatically prunes old Docker/Podman images after deployment to save disk space
# while keeping recent versions for rollbacks. Configure retention per service:
#
# retain: <number>   # Number of recent images to keep (default: 3)
#
# Image Retention Examples:
# - retain: 3        # Keep last 3 images (default)
# - retain: 5        # Keep last 5 images for critical services
# - retain: 1        # Keep only the latest image for non-critical services
#
# How It Works:
# 1. After each deployment, Jiji identifies all images for each service
# 2. Sorts images by creation time (newest first)
# 3. Keeps the N most recent images (based on retain setting)
# 4. Removes older images that are not currently in use
# 5. Also removes dangling (untagged) images
#
# Manual Pruning:
# Run 'jiji prune' to manually clean up images outside of deployment
# Use 'jiji prune --retain 5' to override the default retention setting
#
services:
  # Example: Web frontend service using pre-built image
  web-frontend:
    image: nginx:latest
    # Server targets: Reference servers by name from the servers section
    hosts: [vps-prod, home-server]
    # Port Mappings: Map host ports to container ports
    # Supported formats:
    #   - "8000"                           # Container port only (host port auto-assigned)
    #   - "8080:8000"                      # host_port:container_port
    #   - "127.0.0.1:8080:80"              # host_ip:host_port:container_port
    #   - "8080:8000/tcp"                  # Specify TCP protocol (default)
    #   - "1900/udp"                       # Container port with UDP protocol
    #   - "53:53/udp"                      # Host:container with UDP
    #   - "127.0.0.1:5353:53/udp"          # Full format with IP and UDP
    # Protocol suffix: /tcp or /udp (optional, defaults to TCP)
    # Port range: 1-65535
    # IP: Valid IPv4 address (bind to specific interface)
    ports:
      - "127.0.0.1:8080:80"
    # Image Retention: Number of images to keep for rollbacks (default: 3)
    # Older images are automatically pruned after deployment
    retain: 5
    # Volumes: Docker/Podman named volumes or host paths
    volumes:
      - "web_storage:/opt/uploads"
      - "./data:/opt/extra_data:ro"
    # Files: Upload from local repo to host .jiji/{project}/files/ before mounting
    # String format: local:remote[:options] where options can be ro, z, or Z
    files:
      - "nginx.conf:/etc/nginx/nginx.conf:ro"
    # Or use hash format for custom permissions and ownership:
    # files:
    #   - local: config/secret.key
    #     remote: /etc/app/secret.key
    #     mode: "0600"
    #     owner: "nginx:nginx"
    #     options: "ro"
    # Directories: Created on host .jiji/{project}/directories/ before mounting
    # String format: local:remote[:options] where options can be ro, z, or Z
    directories:
      - "html:/usr/share/nginx/html:ro"
    # Or use hash format for custom permissions and ownership:
    # directories:
    #   - local: logs
    #     remote: /var/log/nginx
    #     mode: "0755"
    #     owner: "nginx:nginx"
    #     options: "z"
    # Service-specific environment (merged with shared environment)
    environment:
      clear:
        NGINX_WORKER_PROCESSES: auto
        NGINX_WORKER_CONNECTIONS: "1024"
    # Optional: Proxy configuration for automatic SSL and routing
    # Single Target Proxy Configuration
    # proxy:
    #   app_port: 80                           # Required: container port to proxy
    #   ssl: false
    #   host: web.myproject.example.com
    #   path_prefix: /api                      # Optional: Path-based routing
    #   healthcheck:
    #     path: /health              # HTTP health check endpoint
    #     interval: 10s

  # Example: API service using custom build with single-target proxy
  # api-backend:
  #   build:
  #     context: ./api                    # Build context (relative to project root)
  #     dockerfile: Dockerfile           # Dockerfile path (relative to context)
  #     args:                            # Build arguments for this service
  #       NODE_ENV: production
  #       API_VERSION: "2.1.0"
  #       BUILD_DATE: "${BUILD_DATE}"    # Environment variable substitution
  #   hosts: [vps-prod, arm-builder]     # Deploy to both amd64 and arm64 servers
  #   ports:
  #     - "3000:3000"
  #   environment:
  #     clear:
  #       NODE_ENV: production
  #       DATABASE_URL: "postgresql://user:pass@192.168.1.103:5432/myapp"
  #       REDIS_URL: "redis://redis.local:6379"
  #     secrets:
  #       - JWT_SECRET
  #       - API_KEY
  #   command: ["node", "dist/server.js"]
  #   volumes:
  #     - "/app/logs:/var/log/app"
  #   # Single Target Proxy - Host-based routing (separate subdomain)
  #   proxy:
  #     app_port: 3000                    # Required: container port
  #     ssl: true
  #     host: api.myproject.example.com
  #     healthcheck:
  #       path: /api/health               # HTTP health check endpoint
  #       interval: 15s
  #       timeout: 10s
  #       deploy_timeout: 60s
  #   # Or with command-based health check (for containers without HTTP endpoints)
  #   # proxy:
  #   #   app_port: 3000
  #   #   ssl: true
  #   #   host: api.myproject.example.com
  #   #   healthcheck:
  #   #     cmd: "test -f /app/ready"     # Command health check (exit code 0 = healthy)
  #   #     cmd_runtime: docker           # Optional: Runtime (docker/podman, defaults to builder.engine)
  #   #     interval: 15s
  #   #     timeout: 10s
  #   #     deploy_timeout: 60s
  #   # Or path-based routing (same domain, different path)
  #   # proxy:
  #   #   app_port: 3000
  #   #   ssl: true
  #   #   host: myproject.example.com
  #   #   path_prefix: /api
  #   #   healthcheck:
  #   #     path: /api/health
  #   #     interval: 15s
  #   #     timeout: 10s
  #   #     deploy_timeout: 60s
  #   # Or combined routing (specific host + path prefix)
  #   # proxy:
  #   #   app_port: 3000
  #   #   ssl: true
  #   #   host: app.myproject.example.com
  #   #   path_prefix: /v2
  #   #   healthcheck:
  #   #     path: /v2/health
  #   #     interval: 15s
  #   #     timeout: 10s
  #   #     deploy_timeout: 60s

  # Example: Multi-Target Proxy for services with multiple ports
  # Useful for services like Garage that expose multiple APIs
  # garage:
  #   image: dxflrs/garage:v2.1.0
  #   servers:
  #     - host: 192.168.1.104
  #       arch: amd64
  #   ports:
  #     - "127.0.0.1:3900:3900"           # S3 API
  #     - "127.0.0.1:3903:3903"           # Admin API
  #   volumes:
  #     - "garage_meta:/var/lib/garage/meta"
  #     - "garage_data:/var/lib/garage/data"
  #   files:
  #     - "garage.toml:/etc/garage.toml"
  #   # Multi-Target Proxy Configuration
  #   proxy:
  #     targets:
  #       - app_port: 3900                # S3 API endpoint
  #         host: s3.garage.example.com
  #         ssl: false
  #         healthcheck:
  #           path: /health               # HTTP health check endpoint
  #           interval: 30s
  #           deploy_timeout: 60s
  #       - app_port: 3903                # Admin API endpoint
  #         host: admin.garage.example.com
  #         ssl: true
  #         healthcheck:
  #           path: /health               # HTTP health check endpoint
  #           interval: 30s
  #           deploy_timeout: 60s
  #   # Note: Each target is deployed as {project}-{service}-{app_port} in kamal-proxy
  #   # Example: myproject-garage-3900, myproject-garage-3903
  #
  # Health Check Configuration:
  # ==========================
  # Jiji supports two types of health checks via kamal-proxy:
  #
  # 1. HTTP Health Checks (default)
  #    - Uses HTTP GET requests to check container health
  #    - Best for web services with HTTP endpoints
  #    - Example:
  #      healthcheck:
  #        path: /health                  # HTTP endpoint to check (e.g., "/health", "/up")
  #        interval: 10s                  # Check interval (default: 10s)
  #        timeout: 5s                    # Request timeout (default: 5s)
  #        deploy_timeout: 60s            # Max wait for initial deployment (default: 60s)
  #
  # 2. Command-based Health Checks (new in kamal-proxy)
  #    - Executes commands via docker exec or podman exec
  #    - Exit code 0 = healthy, any other exit code = unhealthy
  #    - Best for non-HTTP services or custom health logic
  #    - Compatible with distroless/scratch containers
  #    - Example:
  #      healthcheck:
  #        cmd: "test -f /app/ready"      # Command to execute (exit 0 = healthy)
  #        cmd_runtime: docker            # Optional: Runtime (docker/podman, auto-detects from builder.engine)
  #        interval: 10s                  # Check interval (default: 10s)
  #        timeout: 5s                    # Command timeout (default: 5s)
  #        deploy_timeout: 60s            # Max wait for initial deployment (default: 60s)
  #
  # Command Health Check Examples:
  # - File-based readiness: cmd: "test -f /app/ready"
  # - Process check: cmd: "pgrep -f myapp"
  # - Custom script: cmd: "/app/healthcheck.sh"
  # - Internal HTTP check: cmd: "curl -f http://localhost:3000/health"
  # - Complex check: cmd: '/app/healthcheck --config "/etc/app.conf"'
  #
  # Notes:
  # - HTTP (path) and command (cmd) health checks are mutually exclusive
  # - If neither is specified, no health checks are performed
  # - Health checks are used for both initial deployment and ongoing monitoring
  # - Failed health checks during deployment keep old containers running (zero-downtime)

  # Admin service with path-based routing
  # admin-panel:
  #   build:
  #     context: ./admin
  #     dockerfile: Dockerfile
  #   servers:
  #     - host: 192.168.1.100
  #       arch: amd64
  #   ports:
  #     - "3001:3000"
  #   environment:
  #     clear:
  #       NODE_ENV: production
  #   proxy:
  #     ssl: true
  #     host: myproject.example.com
  #     path_prefix: /admin

# Server Architecture Configuration Examples:
#
# 1. Single architecture deployment:
#   servers:
#     - host: 192.168.1.100
#       arch: amd64
#     - host: 192.168.1.101
#       arch: amd64
#
# 2. Mixed architecture deployment:
#   servers:
#     - host: 192.168.1.100
#       arch: amd64
#     - host: 192.168.1.101
#       arch: arm64
#
# 3. ARM-only deployment:
#   servers:
#     - host: 192.168.1.200
#       arch: arm64
#     - host: 192.168.1.201
#       arch: arm64
#
# Notes:
# - Each server must specify host and optionally arch
# - Supported architectures: amd64, arm64
# - If no arch specified, defaults to amd64
# - Jiji automatically builds multi-architecture images when needed
#     healthcheck:
#       path: /admin/health
#       interval: 10s

# GraphQL API with path-based routing
# graphql-api:
#   build:
#     context: ./graphql
#     dockerfile: Dockerfile
#   servers:
#     - host: 192.168.1.101
#       arch: amd64
#   ports:
#     - "4000:4000"
#   environment:
#     clear:
#       NODE_ENV: production
#   proxy:
#     ssl: true
#     host: myproject.example.com
#     path_prefix: /graphql
#     healthcheck:
#       path: /graphql/health
#       interval: 15s

# Example: Worker service for background jobs
# worker-queue:
#   build:
#     context: ./worker
#     dockerfile: Dockerfile
#   servers:
#     - host: 192.168.1.103
#       arch: amd64
#   environment:
#     clear:
#       QUEUE_URL: "redis://192.168.1.104:6379"
#       WORKER_CONCURRENCY: "4"
#   command: ["npm", "run", "worker"]
#   volumes:
#     - "/app/worker-data:/data"

# Example: Database service
# database:
#   image: postgres:15-alpine
#   servers:
#     - host: 192.168.1.103
#       arch: amd64
#   ports:
#     - "127.0.0.1:5432:5432"
#   environment:
#     clear:
#       POSTGRES_DB: myapp
#       POSTGRES_USER: appuser
#       POSTGRES_PASSWORD: securepassword
#   volumes:
#     - "/data/postgres:/var/lib/postgresql/data"

# Example: Cache service
# cache:
#   image: redis:7-alpine
#   servers:
#     - host: 192.168.1.104
#       arch: amd64
#   ports:
#     - "127.0.0.1:6379:6379"
#   volumes:
#     - "/data/redis:/data"
#   command: ["redis-server", "--appendonly", "yes"]
#
# # Example: DNS server with UDP ports
# dns-server:
#   image: coredns/coredns:latest
#   servers:
#     - host: 192.168.1.105
#       arch: amd64
#   ports:
#     - "53:53/udp"              # DNS queries over UDP
#     - "53:53/tcp"              # DNS zone transfers over TCP
#   volumes:
#     - "./coredns/Corefile:/etc/coredns/Corefile:ro"
#   command: ["-conf", "/etc/coredns/Corefile"]
#
# # Example: Service with mixed TCP/UDP ports
# game-server:
#   image: game-server:latest
#   servers:
#     - host: 192.168.1.106
#       arch: amd64
#   ports:
#     - "8080:8080/tcp"          # HTTP API
#     - "7777:7777/udp"          # Game protocol
#     - "7778:7778/udp"          # Voice chat
#     - "127.0.0.1:9000:9000"    # Admin interface (localhost only)
#
# # Example: ML service with GPU and resource constraints
# ml-trainer:
#   image: tensorflow/tensorflow:latest-gpu
#   servers:
#     - host: 192.168.1.107
#       arch: amd64
#   ports:
#     - "8888:8888"              # Jupyter notebook
#   # Resource Constraints:
#   cpus: 4                      # Limit to 4 CPUs (can be fractional: 0.5, 1.5, etc.)
#   memory: "8g"                 # Memory limit (e.g., "512m", "1g", "2gb", "1024mb")
#   gpus: "all"                  # GPU access: "all", "0", "0,1", "device=0"
#   volumes:
#     - "ml_models:/models"
#     - "ml_data:/data"
#   environment:
#     clear:
#       CUDA_VISIBLE_DEVICES: "0"
#
# # Example: Media server with device access
# media-processor:
#   image: ffmpeg:latest
#   servers:
#     - host: 192.168.1.108
#       arch: amd64
#   ports:
#     - "8080:8080"
#   # Device Mappings: Grant access to host devices
#   devices:
#     - "/dev/video0"            # Simple device mapping
#     - "/dev/video1:/dev/video1:rwm"  # With permissions (r=read, w=write, m=mknod)
#     - "/dev/snd"               # Audio devices
#   cpus: 2
#   memory: "2g"
#   volumes:
#     - "media_input:/input"
#     - "media_output:/output"
#
# # Example: FUSE filesystem with privileged mode
# rclone-mount:
#   image: rclone/rclone:latest
#   servers:
#     - host: 192.168.1.109
#       arch: amd64
#   ports:
#     - "5572:5572"              # WebUI
#   # Privileged mode: Grants extended privileges to container
#   privileged: true             # Required for FUSE mounts
#   devices:
#     - "/dev/fuse"              # FUSE device access
#   memory: "2g"
#   volumes:
#     - "rclone_config:/config"
#     - "rclone_cache:/cache"
#   command: ["rcd", "--rc-web-gui", "--rc-addr", ":5572"]
#
# # Example: Network service with specific capabilities
# vpn-server:
#   image: linuxserver/wireguard:latest
#   servers:
#     - host: 192.168.1.110
#       arch: amd64
#   ports:
#     - "51820:51820/udp"        # WireGuard
#   # Linux Capabilities: Add specific capabilities instead of full privileged mode
#   cap_add:
#     - NET_ADMIN                # Network administration operations
#     - SYS_MODULE               # Load/unload kernel modules
#   devices:
#     - "/dev/net/tun"           # TUN device for VPN
#   environment:
#     clear:
#       PEERX_PUBLIC_KEY: "your-public-key"
#       SERVERURL: "vpn.example.com"
#
# Resource Constraints Reference:
# ==============================
#
# cpus: Number of CPUs to allocate
#   - Type: number or string
#   - Examples: 0.5, 1, 2, 4, "1.5"
#   - Limits CPU usage for the container
#
# memory: Memory limit for the container
#   - Type: string with format: number + unit
#   - Units: b, k, m, g, kb, mb, gb (case-insensitive)
#   - Examples: "512m", "1g", "2gb", "1024mb"
#   - Prevents container from using more than specified memory
#
# gpus: GPU device access
#   - Type: string
#   - Examples:
#     - "all" - Access to all available GPUs
#     - "0" - Access to GPU device 0
#     - "0,1" - Access to GPU devices 0 and 1
#     - "device=0" - Specific device notation
#   - Note: Requires NVIDIA Container Toolkit (Docker) or equivalent (Podman)
#
# devices: Device mappings for hardware access
#   - Type: array of strings
#   - Formats:
#     - "/dev/video0" - Simple mapping (host:container same path)
#     - "/dev/video0:/dev/video0" - Explicit host:container mapping
#     - "/dev/video0:/dev/video0:rwm" - With permissions
#   - Permissions (optional): r (read), w (write), m (mknod)
#   - Common devices:
#     - Video: /dev/video0, /dev/video1
#     - Audio: /dev/snd
#     - GPU: /dev/nvidia0, /dev/nvidiactl
#     - USB: /dev/bus/usb
#     - FUSE: /dev/fuse
#     - TUN: /dev/net/tun
#
# privileged: Run container in privileged mode
#   - Type: boolean
#   - Default: false
#   - Examples: true, false
#   - Grants extended privileges to the container
#   - Security Warning: Gives container nearly all capabilities of the host
#   - Use Cases:
#     - FUSE filesystems (with /dev/fuse device)
#     - Docker-in-Docker
#     - Low-level system operations
#   - Alternative: Use cap_add for specific capabilities when possible
#
# cap_add: Add Linux capabilities to container
#   - Type: array of strings
#   - Default: [] (empty)
#   - Examples: ["SYS_ADMIN"], ["NET_ADMIN", "SYS_MODULE"]
#   - Grants specific Linux capabilities without full privileged mode
#   - More secure than privileged mode - principle of least privilege
#   - Common capabilities:
#     - SYS_ADMIN: Mount filesystems, FUSE operations, namespace operations
#     - NET_ADMIN: Network configuration, routing, firewall rules
#     - NET_RAW: Use RAW and PACKET sockets
#     - SYS_MODULE: Load/unload kernel modules
#     - SYS_PTRACE: Debug processes, trace system calls
#     - SYS_TIME: Set system clock
#     - MKNOD: Create special files (device nodes)
#   - Full list: man 7 capabilities
#   - Note: Capability names should NOT include "CAP_" prefix
#
# stop_first: Stop old container before starting new one
#   - Type: boolean
#   - Default: false
#   - When false: Zero-downtime deployment (rename old → start new → stop old)
#   - When true: Stop-first deployment (stop old → start new)
#   - Use for stateful services that cannot have two instances running:
#     - Databases with file locks (SQLite, LevelDB, embedded stores)
#     - Services registering unique IDs (volume servers, cluster members)
#     - Services with exclusive resource locks (FUSE mounts, GPU memory)
#   - Trade-off: Causes brief downtime but prevents resource conflicts
#   - Warning: No rollback possible if new container fails to start
#   - Example:
#     services:
#       database:
#         image: postgres:15
#         stop_first: true  # Database can't have two instances with same data dir
#       filer:
#         image: seaweedfs/seaweedfs:latest
#         stop_first: true  # LevelDB requires exclusive lock
#

# Proxy Routing Configuration:
# ============================
#
# Jiji supports both host-based and path-based routing:
#
# Host-based routing (different domains/subdomains):
# services:
#   frontend:
#     proxy:
#       ssl: true
#       host: myapp.com
#
#   api:
#     proxy:
#       ssl: true
#       host: api.myapp.com
#
# Path-based routing (same domain, different paths):
# services:
#   frontend:
#     proxy:
#       ssl: true
#       host: myapp.com          # Catch-all
#
#   api:
#     proxy:
#       ssl: true
#       host: myapp.com
#       path_prefix: /api        # Routes myapp.com/api/*
#
#   admin:
#     proxy:
#       ssl: true
#       host: myapp.com
#       path_prefix: /admin      # Routes myapp.com/admin/*
#
# Combined routing (host + path):
# services:
#   api-v1:
#     proxy:
#       ssl: true
#       host: api.myapp.com
#       path_prefix: /v1         # Routes api.myapp.com/v1/*
#
# Routing priority:
# - Longer path prefixes match first (/api/v2 before /api)
# - Services without path_prefix are catch-all for their host

# Service Build Architecture Configuration:
# ========================================
# The 'arch' field in service build configurations specifies the target architecture(s)
# for container builds. This enables multi-platform deployments and cross-architecture builds.
#
# Key Features:
# - Single architecture: arch: "amd64" (default if not specified)
# - Multi-architecture: arch: ["amd64", "arm64"]
# - Automatic platform detection: Uses Docker/Podman --platform flags
# - Cross-compilation support: Build ARM images on Intel machines (and vice versa)
#
# Supported Architectures:
# - amd64: Intel/AMD 64-bit (most common server architecture)
# - arm64: ARM 64-bit (Apple Silicon, AWS Graviton, Raspberry Pi 4+)
#
# Docker/Podman Requirements:
# - Docker: Requires Docker Desktop with buildx or Docker CE with buildx plugin
# - Podman: Native multi-arch support in recent versions
# - For multi-arch builds, the base images must support the target architectures
#
# Use Cases:
# 1. Consistent amd64 builds for Intel/AMD servers
# 2. ARM64 builds for Apple Silicon development and ARM servers
# 3. Multi-arch builds for hybrid cloud deployments
# 4. Cross-platform container distribution

# Service Build Architecture Examples:
# ===================================
# Single architecture build (amd64 is default):
# services:
#   web-app:
#     build:
#       context: .
#       # arch: amd64  # Optional - amd64 is default
#
# Multi-architecture build:
# services:
#   web-app:
#     build:
#       context: .
#       arch:
#         - amd64
#         - arm64
#
# Mixed architecture deployments:
# services:
#   web-frontend:
#     build:
#       context: ./frontend
#       # arch: amd64            # Default architecture (Intel servers)
#     hosts:
#       - web1.example.com
#
#   api-backend:
#     build:
#       context: ./backend
#       arch:
#         - amd64
#         - arm64              # Support both Intel and ARM servers
#     hosts:
#       - api1.example.com    # Intel server
#       - api2.example.com    # ARM server
#
# Architecture-specific optimization example:
# services:
#   ml-service:
#     build:
#       context: ./ml-service
#       dockerfile: Dockerfile.gpu
#       arch: amd64           # GPU workloads typically require Intel/AMD
#     hosts:
#       - gpu-server.example.com
#
#   iot-gateway:
#     build:
#       context: ./iot-gateway
#       dockerfile: Dockerfile.arm
#       arch: arm64           # IoT deployments often use ARM
#     hosts:
#       - iot-hub.example.com
#
# Cross-platform development example:
# services:
#   web-app:
#     build:
#       context: .
#       arch:
#         - amd64             # Production Intel servers
#         - arm64             # Local Apple Silicon development
#     hosts:
#       - prod-web1.example.com   # Intel production
#       - prod-web2.example.com   # Intel production

# Service Deployment Filtering Examples:
# =====================================
# Deploy specific services:           jiji deploy --services web-frontend,api-backend
# Deploy by pattern:                  jiji deploy --patterns "web-*,*-backend"
# Deploy build-only services:         jiji deploy --build-only
# Deploy image-only services:         jiji deploy --image-only
# Deploy to specific hosts:           jiji deploy --target-hosts 192.168.1.100,192.168.1.101
# Exclude services:                   jiji deploy --exclude worker-queue,cache
# Combine filters:                    jiji deploy --patterns "web-*" --exclude web-admin --target-hosts 192.168.1.100
